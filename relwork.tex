%
% relwork.tex
%

Several projects have attempted to solve the problem of data visualization and analysis on the web, including a number focused on natural systems. Huang et al. \cite{gisvis} developed server-side and client-side hybrid approaches that take advantage of a strong server-side solution. This allows the client to be served a universal HTML source that is compatible with all browsers and complies with web standards with minimal effort.  Furthermore, such a system allows all administration, processing, and data to be centralized at the server. Server-side solutions are built using more mature and simplistic technology, using the server to render a visualization and serve it to the client. However, a server-side solution comes with disadvantages, namely it offers a far less interactive and user-friendly solution and results in a large number of server requests.  Huang et al. suggest that a client-side solution implemented using ActiveX or Java offers a more interactive experience for the user while offloading performance demands to the client.  The client is sent data for processing and visualization, removing processing tasks from the server. Client-side solutions are typically less demanding on an the client’s persistent internet connection and are less demanding of the server.  Client-side solutions have a major downside as they require downloading client-side software, or at least JavaScript source files for modern web applications. In the case of JavaScript web applications, some form of client-side code is released to the client for open access, whereas a server-heavy solution prevents the release of proprietary software to the public. \par
Thorvaldsdóttir et al. \cite{igv} developed a visualization program that allows data from remote sources.  The software utility handles large and diverse data sets, allowing for different scaled views to focus on small subsets of data or view the data as a whole, and allow the user a means of comparative analysis.  This software architecture takes a layered approach to allow for various functionality such as access to different local and remote data sets and generating different views for data sets. The architecture contains a steaming layer, which provides access to different parts of a dataset, both remote and local.  The data layer handles the ingestion and management of imported datasets from different supported formats, and the application layer provides the user interface.  This architecture is reminiscent of the MVC architecture. As part of the application layer, they allow the user to view the dataset with highlighted features and focus on certain portions of the desired dataset.  Their use of color to highlight important features and allow for quick navigation is of notable importance. \par
Wood et al. \cite{neuro} overviews a web-based system for sharing and visualizing neuroimaging data.  This data can be queried, viewed, and downloaded.  They discuss a specific interface for query building that has the user build a query from certain building blocks representing data sources and operations.  This interface presents an interesting potential for data filtering, grouping, and analysis as it allows the user freedom to control what data is retrieved and how the data is analyzed.  This method can be further extended with new innovations in real-time data synchronization between the server and the client.  Their architecture involves an Apache server which serves from processed PHP scripts in order to authenticate the user and set up their session.  These web pages then interact with a HTTP interface to a Node.js server to retrieve available query elements and query results.  Interactions with JSON data, the complexities involved, and the methods for sending large datasets through a serialized connection are also discussed.  These methods have evolved since their paper was written, but the principles remain the same. \par
Ma et al. \cite{smartbuildings} introduced a monitoring system for Internet of Things (IOT) devices built on top of WebSockets using Socket.io, a Node.js server, a MongoDB database, and a web portal for data access which uses Chart.js for data visualization.  They propose the use of WebSockets through Socket.IO as the single means of bi-directional communication between a server and a device and a client.  In this model, the server both ingests data from IOT devices into the MongoDB database and serves data to a client from the database.  This architecture allows the user to both access data from the device and control the IOT device through a single web server.  The client has access to real-time and historical data that can be visualized using packages such as Chart.js.  Chart.js utilizes the HTML5 canvas and JavaScript to create interactive charts for the large datasets served to the client via a WebSocket.  Another feature of note in the proposed architecture is the NOSQL database solution - MongoDB, which was selected for its balance of read/write performance and its ability to easily handle large datasets.  Ma et al. \cite{smartbuildings}  define a very adaptable and general architecture for efficient IOT connectivity, data transfer, control, and data visualization. \par
Cawthon et al. \cite{aesthetic} explore the effect of aesthetics and style on the effectiveness of a data visualization.  They assembled an online study to test the effectiveness of different visualizations and the rated aesthetic quality of a given visualization.  Their goal was to identify correlations between the rated aesthetic beautify of a visualization and its ability to correctly and efficiently convey information.  To do so, they first asked 285 valid users to “reflect on the aesthetic quality of the image” as they “would with a painting or illustration”.  Following an aesthetic ranking, they asked the users 14 questions, 2 questions for each of 7 visualizations, based on the data and rated their responses on correctness, response time, and task abandonment.  Specifically, they allowed the user to abandon their task if it was found very difficult.  A visualization that is aesthetically appealing was found to effectively portray the data and may also encourage the user to spend more time analyzing the visualization.  They mentioned that one of the techniques, Sunburst, which ranked very aesthetically appealing had very low abandonment and a high rate of correct responses.  They state that “These results prove that participants who did not immediately locate the correct answer felt encouraged to continue their task.”  However, they noticed that two of the visualizations ranked very visually unappealing had the highest accuracy and speed ratings.  Their results also point out that their choice of color palette as greens, blues, and browns were pleasing to the human eye.  These observations and results are useful to keep in mind when developing adaptable visualizations of complex data that could easily become overwhelming. \par
Gomez et al. \cite{biojs} present BioJS, an open source JavaScript framework for the creation and use of biological data visualizations.  Their primary goal was to create a framework for others to build upon to create various reusable visualizations.  Specifically, they opened the framework entirely for extension by the community.  They defined a framework that, once learned, would allow other developers to easily create new reusable visualizations as components.  BioJS, as a framework, simply defines the component architecture, the protocol for event handling and communication between components, the component extension through Object-Oriented Inheritance, documentation format, and documentation on how to include examples and to test the component functionality.  Their framework seems to provide a sufficient base for a coherent and extendable visualization framework, meanwhile taking a hands-off approach and allowing other frameworks, packages, and plotting libraries to be used.  As they say, they are “platform agnostic”.  Their project continues to this day and provides a number of JavaScript-based web visualizations for biological data. \par
Wessels et al. \cite{remotevis} defined a framework for remotely rendering and streaming data visualizations.  Their architecture defines four primary “layers” or, as they call them, components.  These components are the Server, Visualization Engine, Daemon, and Client.  The Server houses the rendering hardware and executes the Visualization Engine and the Daemon.  The Visualization engine is tasked with rendering images to be sent to the client.  The Daemon runs continuously and spawns visualization processes as requested.  The Daemon would now be known as the web server as it controls all socket interactions and the provision of resources as requests are received.  The Client is directly sent rendered images of a data visualization through a data stream to an HTML Canvas.  The user can interact with these visualization through the HTML Canvas.  If the user interacts with a visualization, for example a 3d image, their actions are sent back to the server, where a new frame is rendered and streamed back to the client.  This architecture was created with the goal of overcoming client-side rendering limitations with large data sets.  The client can slow down or even crash if given too many points to render.  This architecture offers an interesting solution by offloading the rendering and data accessing tasks to the server that also stores the data and streams the rendered visualization to the user.  This architecture can be augmented to employ many of the other architectures discussed earlier, but this architecture focuses rendering tasks to the server. Currently, it is inadvisable to focus performance demands on the web server, as web server costs could be preventative to smaller projects. \par
The D3: Data-Driven Documents introduced by Bostock et al. \cite{d3} outlines 3 specific objectives: compatibility, debugging, and performance.  Their work towards compatibility led to a focus on interoperability with outside tools and reusability through extension with a method to directly access the native representation of the visualization.  Their focus on debugging led to a focus on the control flow, encouraging them to allow the developer to interact with every layer of the control flow.  Their performance objective resulted in a focus on transformation.  By focusing on transformation between states, you can avoid doing redundant work to re-render data that hasn’t changed.  This also resulted in an affinity for interactivity.  They provide an overview of their design choices related to the selection of DOM elements, their work towards an intuitive mapping of data to visual elements, their focus on transitions, interactions, and animation, and their focus on a modular design providing basic functionality while maintaining extendibility. \par
Figueiras et al. \cite{interaction} explores 11 interaction techniques used in data visualizations across various web visualization sources.  Specifically, the techniques overviewed are filtering, selecting, abstract/elaborate, overview and explore, connect/relate, reconfigure, encode, history, extraction of features, participation/collaboration, and gamification. Each of these interactivity techniques contribute to the user’s ability to locate information with a visualization and encourage the user to continue interacting with a visualization.  For example, a user might filter a visualization to ignore a dataset which is unrelated to their target information.  Another example might focus on the exploring the data by allowing a user to pan across a dataset or load and unload specific relevant datasets across a visualization.  Figueiras et al. provide a detailed overview of the various uses of interactivity in a visualization and overview the benefits of interaction including improved user experience, engagement, and effectiveness.  These interaction techniques should be permitted for and encouraged by any web-based data visualization architecture. \par
